import logging
import os
import numpy as np
import torch
import scipy.io as sio

from ignite.engine import Events
from ignite.handlers import ModelCheckpoint
from ignite.handlers import Timer

from engine.engine import create_eval_engine
from engine.engine import create_train_engine
from engine.metric import AutoKVMetric
from utils.eval_sysu import eval_sysu
from utils.eval_regdb import eval_regdb
from utils.eval_llcm import eval_llcm
from configs.default.dataset import dataset_cfg
from configs.default.strategy import strategy_cfg

def get_trainer(dataset, model, optimizer, lr_scheduler=None, logger=None, writer=None, non_blocking=False, log_period=10,
                save_dir="checkpoints", prefix="model", gallery_loader=None, query_loader=None,
                eval_interval=None, start_eval=None, rerank=False, fp16=False):
    if logger is None:
        logger = logging.getLogger()
        logger.setLevel(logging.WARN)

    # trainer
    trainer = create_train_engine(model, optimizer, non_blocking, fp16=fp16)

    setattr(trainer, "rerank", rerank)

    # checkpoint handler
    # handler = ModelCheckpoint(save_dir, prefix, save_interval=eval_interval, n_saved=3, create_dir=True,
    #                           save_as_state_dict=True, require_empty=False)
    handler = ModelCheckpoint(save_dir, prefix, n_saved=3, create_dir=True,
                          require_empty=False,
                          global_step_transform=lambda engine, _: engine.state.epoch)
    
    # handler = ModelCheckpoint(save_dir, prefix, n_saved=3, create_dir=True,
    #                           require_empty=False) #Add

    trainer.add_event_handler(Events.EPOCH_COMPLETED, handler, {"model": model})

    # metric
    timer = Timer(average=True)
    rank = True

    kv_metric = AutoKVMetric()

    # evaluator
    evaluator = None
    if not type(eval_interval) == int:
        raise TypeError("The parameter 'validate_interval' must be type INT.")
    if not type(start_eval) == int:
        raise TypeError("The parameter 'start_eval' must be type INT.")
    
    # print("Gallery not none") if gallery_loader is not None else print("Gallery none")
    # print("query not none") if query_loader is not None else print("query none")
        
    if eval_interval > 0 and gallery_loader is not None and query_loader is not None:
        evaluator = create_eval_engine(model, non_blocking)
        # print(f"value from create_eval_engine: {type(evaluator)}")

    @trainer.on(Events.STARTED)
    def train_start(engine):
        setattr(engine.state, "best_rank1", 0.0)
        setattr(engine.state, "patience_counter", 0) #early stopping

    @trainer.on(Events.COMPLETED)
    def train_completed(engine):
        torch.cuda.empty_cache()

        # extract query feature
        evaluator.run(query_loader)

        q_feats = torch.cat(evaluator.state.feat_list, dim=0)
        q_ids = torch.cat(evaluator.state.id_list, dim=0).numpy()
        q_cams = torch.cat(evaluator.state.cam_list, dim=0).numpy()
        q_img_paths = np.concatenate(evaluator.state.img_path_list, axis=0)

        # extract gallery feature
        evaluator.run(gallery_loader)

        g_feats = torch.cat(evaluator.state.feat_list, dim=0)
        g_ids = torch.cat(evaluator.state.id_list, dim=0).numpy()
        g_cams = torch.cat(evaluator.state.cam_list, dim=0).numpy()
        g_img_paths = np.concatenate(evaluator.state.img_path_list, axis=0)

        logger.info("best rank1={:.2f}%".format(engine.state.best_rank1))

        if dataset == 'sysu':
            # perm = sio.loadmat(os.path.join(dataset_cfg.sysu.data_root, 'exp', 'rand_perm_cam.mat'))[
            #     'rand_perm_cam']
            perm = sio.loadmat("./data_dir/SYSU-MM01/exp/rand_perm_cam.mat")[
                'rand_perm_cam']
            eval_sysu(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, perm, mode='all', num_shots=1, rerank=engine.rerank)
            eval_sysu(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, perm, mode='all', num_shots=10, rerank=engine.rerank)
            eval_sysu(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, perm, mode='indoor', num_shots=1, rerank=engine.rerank)
            eval_sysu(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, perm, mode='indoor', num_shots=10, rerank=engine.rerank)
        elif dataset == 'regdb':
            print('infrared to visible')
            eval_regdb(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, rerank=engine.rerank)
            print('visible to infrared')
            eval_regdb(g_feats, g_ids, g_cams, q_feats, q_ids, q_cams, q_img_paths, rerank=engine.rerank)
        elif dataset == 'llcm':
            print('infrared to visible')
            eval_llcm(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, rerank=engine.rerank)
            print('visible to infrared')
            eval_llcm(g_feats, g_ids, g_cams, q_feats, q_ids, q_cams, q_img_paths, rerank=engine.rerank)


        evaluator.state.feat_list.clear()
        evaluator.state.id_list.clear()
        evaluator.state.cam_list.clear()
        evaluator.state.img_path_list.clear()
        del q_feats, q_ids, q_cams, g_feats, g_ids, g_cams

        torch.cuda.empty_cache()

    @trainer.on(Events.EPOCH_STARTED)
    def epoch_started_callback(engine):
    
        epoch = engine.state.epoch
        if model.mutual_learning:
            model.update_rate = min(100 / (epoch + 1), 1.0) * model.update_rate_

        kv_metric.reset()
        timer.reset()

    @trainer.on(Events.EPOCH_COMPLETED)
    def epoch_completed_callback(engine):
        epoch = engine.state.epoch

        if lr_scheduler is not None:
            lr_scheduler.step()
            
        if epoch % 1 == 0:
            metric_dict = kv_metric.compute()
            msg = f"Epoch[{epoch}] Final Metrics: "
            for k, v in sorted(metric_dict.items()):
                msg += f"{k}: {v:.4f} "
            logger.info(msg)

        if epoch % eval_interval == 0:
            # logger.info(f"Epoch: {epoch}")
            logger.info("Model saved at {}/{}_model_{}.pth".format(save_dir, prefix, epoch))

        if evaluator and epoch % eval_interval == 0 and epoch >= start_eval:
            torch.cuda.empty_cache()

            # extract query feature
            print(f"Query loader length: {len(query_loader)}")
            for i, batch in enumerate(query_loader):
                # print(f"Batch {i}: images shape: {batch['images'].shape if 'images' in batch else 'No images key'}")
                if i > 2:  
                    break
            evaluator.run(query_loader)
            print(f"After query run: feat_list length: {len(evaluator.state.feat_list)}")
            if evaluator.state.feat_list:
                print(f"First feat shape: {evaluator.state.feat_list[0].shape}")
            else:
                print("Error: feat_list is empty after query run!")

            q_feats = torch.cat(evaluator.state.feat_list, dim=0)
            q_ids = torch.cat(evaluator.state.id_list, dim=0).numpy()
            q_cams = torch.cat(evaluator.state.cam_list, dim=0).numpy()
            q_img_paths = np.concatenate(evaluator.state.img_path_list, axis=0)

            # extract gallery feature
            print(f"Gallery loader length: {len(gallery_loader)}")
            for i, batch in enumerate(gallery_loader):
                # print(f"Batch {i}: images shape: {batch['images'].shape if 'images' in batch else 'No images key'}")
                if i > 2:  
                    break
            evaluator.run(gallery_loader)
            print(f"After gallery run: feat_list length: {len(evaluator.state.feat_list)}")
            if evaluator.state.feat_list:
                print(f"First feat shape: {evaluator.state.feat_list[0].shape}")
            else:
                print("Error: feat_list is empty after gallery run!")

            g_feats = torch.cat(evaluator.state.feat_list, dim=0)
            g_ids = torch.cat(evaluator.state.id_list, dim=0).numpy()
            g_cams = torch.cat(evaluator.state.cam_list, dim=0).numpy()
            g_img_paths = np.concatenate(evaluator.state.img_path_list, axis=0)

            if dataset == 'sysu':
                # perm = sio.loadmat(os.path.join(dataset_cfg.sysu.data_root, 'exp', 'rand_perm_cam.mat'))[
                #     'rand_perm_cam']
                perm = sio.loadmat("./data_dir/SYSU-MM01/exp/rand_perm_cam.mat")[
                    'rand_perm_cam']
                mAP, r1, r5, _, _ = eval_sysu(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, perm, mode='all', num_shots=1, rerank=engine.rerank)
            elif dataset == 'regdb':
                print('infrared to visible')
                mAP, r1, r5, _, _ = eval_regdb(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, rerank=engine.rerank)
                print('visible to infrared')
                mAP, r1_, r5, _, _ = eval_regdb(g_feats, g_ids, g_cams, q_feats, q_ids, q_cams, q_img_paths, rerank=engine.rerank)
                r1 = (r1 + r1_) / 2
            elif dataset == 'llcm':
                print('infrared to visible')
                mAP, r1, r5, _, _ = eval_llcm(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, rerank=engine.rerank)
                #new_all_cmc,mAP, _ = eval_llcm(q_feats, q_ids, q_cams, g_feats, g_ids, g_cams, g_img_paths, rerank=engine.rerank)
                print('visible to infrared')
                mAP, r1_, r5, _, _ = eval_llcm(g_feats, g_ids, g_cams, q_feats, q_ids, q_cams, q_img_paths, rerank=engine.rerank)
                r1 = (r1 + r1_) / 2

                # new_all_cmc,mAP_, _= eval_llcm(g_feats, g_ids, g_cams, q_feats, q_ids, q_cams, q_img_paths,
                #                                rerank=engine.rerank)
                # r1 = (mAP + mAP_) / 2
                # import pdb
                # pdb.set_trace()
                
            patience_limit = getattr(dataset_cfg, 'early_stopping_patience', 20)
            
            if r1 > engine.state.best_rank1:
                engine.state.best_rank1 = r1
                engine.state.patience_counter = 0
                torch.save(model.state_dict(), "{}/model_best.pth".format(save_dir))
                logger.info(f"New Best Rank-1: {r1:.2f}%. Patience reset to 0.")
            else:
                engine.state.patience_counter += 1
                logger.info(f"Patience: {engine.state.patience_counter}/{patience_limit}")
                
                if engine.state.patience_counter >= patience_limit:
                    logger.warn(f"Early stopping at Epoch {epoch}. Best Rank-1: {engine.state.best_rank1:.2f}%")
                    engine.terminate()

            if writer is not None:
                writer.add_scalar('eval/mAP', mAP, epoch)
                writer.add_scalar('eval/r1', r1, epoch)
                writer.add_scalar('eval/r5', r5, epoch)

            evaluator.state.feat_list.clear()
            evaluator.state.id_list.clear()
            evaluator.state.cam_list.clear()
            evaluator.state.img_path_list.clear()
            del q_feats, q_ids, q_cams, g_feats, g_ids, g_cams

            torch.cuda.empty_cache()

    @trainer.on(Events.ITERATION_COMPLETED)
    def iteration_complete_callback(engine):
        timer.step()

        # print(engine.state.output)
        kv_metric.update(engine.state.output)

        epoch = engine.state.epoch
        iteration = engine.state.iteration
        iter_in_epoch = iteration - (epoch - 1) * len(engine.state.dataloader)

        if iter_in_epoch % log_period == 0 and iter_in_epoch > 0:
            batch_size = engine.state.batch[0].size(0)
            speed = batch_size / timer.value()

            msg = "Epoch[%d] Batch [%d]\tSpeed: %.2f samples/sec" % (epoch, iter_in_epoch, speed)

            metric_dict = kv_metric.compute()

            # log output information
            if logger is not None:
                for k in sorted(metric_dict.keys()):
                    msg += "\t%s: %.4f" % (k, metric_dict[k])
                    if writer is not None:
                        writer.add_scalar('metric/{}'.format(k), metric_dict[k], iteration)

                logger.info(msg)

            kv_metric.reset()
            timer.reset()

    return trainer
